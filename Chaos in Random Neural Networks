\documentclass[a4paper,11pt]{article}
\usepackage{jheppub} % for details on the use of the package, please see the JINST-author-manual
\usepackage{lineno}
\usepackage{amsmath}
\usepackage{tensor}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{graphicx}
\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}


\title{\boldmath A Detailed Review of the Extended Mean-Field Theory for Recurrent Networks of Rate Units, Perturbation Theory and Tethering on the Edge of Chaos}


\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{KEYWORDS:---}} #1
}

\author[1]{H. Akmal}
\emailAdd{24100232@lums.edu.pk}
\affiliation{PHY-316, Department of Physics, SBASSE, LUMS.}


\abstract{\\
\\
This research paper explores the application of mean field theory and perturbation theory in the context of recurrent networks modeled using rate units. Recurrent networks are widely used to capture the dynamics of complex systems, including neural networks. By utilizing mean field theory, we approximate the interactions between network elements, considering the average or mean effect of their connectivity. Perturbation theory is then employed to analyze the behavior of the network under small disturbances or deviations from the idealized mean field. We investigate the transition to chaotic states in these networks, where a small perturbation induces a long-lasting effect. The study highlights the phenomenon of critical slowing down, where the system approaches a critical point, exhibiting slower relaxation times and increased sensitivity to perturbations. Through the analysis of the Fokker-Planck equation and the characterization of the equilibrium distribution, we gain insights into the statistical properties of network dynamics near the chaotic regime. This research contributes to our understanding of the dynamics and stability of recurrent networks, shedding light on the emergence of chaos and the effects of perturbations, which have implications for various fields, including neuroscience and the study of complex systems.


}

\keywords{Mean Field Theory, Perturbation Theory, Chaos modelling, Variational methods, Fokker-Planck, Equilibrium distribution}

\begin{document}
\maketitle
\flushbottom
\pagenumbering{arabic}
\newpage
\section{Understanding Mean Field Theory; an intuition}
\label{sec:intro}

Mean field theory is a theoretical framework used in statistical physics and related fields to approximate the behavior of a system consisting of many interacting components. It provides a simplified description by assuming that each component interacts with an average or mean field generated by the other components, neglecting the detailed correlations between them. This approximation is particularly useful when the interactions between the components are weak or the system is large, making it computationally challenging to consider all pairwise interactions.
\\

Let's consider a system composed of N identical components, such as atoms or spins, which can exist in different states. The state of each component is described by a variable called the order parameter, denoted by $\psi$. The goal is to determine the behavior of the system by finding the most probable values of the order parameter under the influence of the mean field.
\\

In mean field theory, the mean field is determined by assuming that each component feels the average effect of the other components, neglecting fluctuations and correlations. Mathematically, this mean field is expressed as:

\begin{equation}
\label{eq:11}
\begin{aligned}
\Psi =  <\psi>. 
\end{aligned}
\end{equation}

where $\Psi$ represents the average value of the order parameter $\psi$, and ⟨ ⟩ denotes the ensemble or spatial average over all components in the system.
\\

The key assumption in mean field theory is that the effect of the mean field on each component is the same, regardless of its position or its own state. This assumption allows us to decouple the system into N independent components, each interacting with the mean field.
\\

The behavior of the system is then described by an effective Hamiltonian or free energy functional, which incorporates the mean field. The effective Hamiltonian depends on the order parameter $\Psi$ and other parameters characterizing the system. The goal is to find the values of $\Psi$ that minimize this effective Hamiltonian, representing the equilibrium state of the system.
\\

To determine the most probable values of $\Psi$, one typically uses variational methods. These methods involve assuming a trial functional form for the order parameter and finding the values of its parameters that minimize the effective Hamiltonian. This minimization process often involves solving equations of motion or optimizing functionals using techniques such as calculus of variations or numerical methods. One popular method is to use the Dyson series for recursive relationships between subsequent order corrections to the error. The perturbation can then be represented as a power series with the corrections subtracted.
\\


Once the equilibrium values of $\Psi$ are obtained, various physical properties of the system can be calculated, such as the energy, specific heat, magnetization, or phase transitions. Mean field theory provides a tractable framework to understand and predict the behavior of complex systems by approximating the effects of interactions through the mean field, despite neglecting correlations and fluctuations.
\\

It's important to note that mean field theory is an approximation, and its validity depends on the specific system under study. It tends to work well in systems with long-range interactions or systems close to a critical point where correlations are long-ranged. However, it may fail to capture important fluctuations and spatial correlations in systems with short-range interactions or in low-dimensional systems. Extensions and refinements to mean field theory, such as higher-order corrections or incorporating fluctuations, have been developed to improve its accuracy in such cases.
\\




\section{Introducing Recurrent Networks and their application to neuroscience}

Recurrent networks, also known as recurrent neural networks (RNNs), are a type of artificial neural network specifically designed to model sequential data or data with temporal dependencies. Unlike feedforward neural networks, which process input data in a single pass from input to output, recurrent networks have connections that allow information to flow in cycles or loops, enabling them to maintain an internal state or memory of past inputs.
\\

The fundamental building block of a recurrent network is the recurrent neuron or cell. Each recurrent neuron receives input not only from the current input data but also from its own output or hidden state at the previous time step. This hidden state serves as the memory of the network, allowing it to capture the temporal information and make predictions or decisions based on the current input and past context.
\\

Mathematically, the hidden state of a recurrent neuron at a given time step t is calculated as a function of the current input and the previous hidden state:
\\

\begin{equation}
\label{eq:2}
\begin{aligned}
h(t) = f(W * x(t) + U * h(t-1))
\end{aligned}
\end{equation}
\\

where h(t) represents the hidden state at time step t, x(t) is the input at time step t, W is the weight matrix connecting the current input to the hidden state, U is the weight matrix connecting the previous hidden state to the current hidden state, and f() is an activation function applied element-wise to the weighted sum.
\\

The recurrence relation in the equation allows the hidden state to capture information from previous time steps, creating a dynamic representation that evolves over time. This property enables recurrent networks to process sequential data of arbitrary length, making them suitable for tasks such as language modeling, speech recognition, machine translation, and time series analysis.
\\

One common type of recurrent network is the Long Short-Term Memory (LSTM) network, which addresses the vanishing gradient problem that can occur in traditional RNNs. The LSTM introduces additional gating mechanisms that regulate the flow of information in and out of the hidden state, allowing the network to selectively remember or forget information over long time intervals.
\\

Another variant is the Gated Recurrent Unit (GRU), which simplifies the LSTM architecture by combining the forget and input gates into a single update gate. The GRU retains much of the LSTM's ability to model long-term dependencies while having a simpler structure with fewer parameters.
\\

Training recurrent networks typically involves optimizing a loss function using gradient-based methods, such as backpropagation through time (BPTT). BPTT extends the backpropagation algorithm to account for the recurrent connections and propagates the gradient information through the time steps, enabling the network to learn from past inputs and adjust its parameters accordingly.
\\

Recurrent networks have demonstrated significant success in modeling sequential data and capturing temporal dependencies. However, they also have limitations, such as difficulties in capturing long-term dependencies or handling very long sequences due to vanishing or exploding gradients. Researchers continue to explore advanced architectures, such as attention mechanisms and transformer-based models, that improve upon recurrent networks for specific tasks while addressing these limitations.
\\

\section{Perturbation Theory in context of Mean Field Theory}
In the context of mean field theory and recurrent networks, perturbation refers to a small deviation or disturbance applied to a system or model. Perturbation theory is a mathematical approach used to analyze the effects of these small perturbations on the behavior of a system and obtain approximate solutions.
\\

In mean field theory, perturbation theory is often employed to study the effects of weak interactions or small deviations from an idealized mean field. By treating the interactions or deviations as perturbations, one can expand the equations describing the system in terms of a parameter representing the strength of the perturbation. This expansion allows for the calculation of approximate solutions and the investigation of how the system's properties change in response to the perturbation.
\\

For example, in the Ising model, a simple lattice model of magnetic spins, mean field theory assumes that each spin interacts with the average or mean field created by all other spins. If one wants to study the effects of a weak external magnetic field, perturbation theory can be used to analyze how the spins respond to this perturbation and determine their average orientations or magnetization.
\\

Similarly, in the context of recurrent networks, perturbation analysis can be used to understand the effects of small disturbances on the dynamics of the network. This analysis allows one to examine how the system's behavior changes when perturbed by external inputs or when the parameters of the network are slightly modified.
\\

By considering the perturbation as a small deviation from the unperturbed or idealized system, one can often simplify the equations governing the system's dynamics and solve them iteratively in terms of a series expansion. The solutions obtained through perturbation theory provide insights into how the system's behavior is affected by the perturbation and can be used to make predictions or gain a better understanding of the system's properties.
\\

Perturbation theory is a powerful tool in theoretical physics and neural network research as it enables the analysis of complex systems under small perturbations, facilitating the study of their behavior, stability, and response to external stimuli.
\\

\section{Transition to the chaotic state in context of chaos in random neural networks - The Edge of Chaos}

The transition to the chaotic state in the context of chaos in random neural networks refers to a phenomenon where the network's dynamics undergo a qualitative change from stable or regular behavior to chaotic behavior as certain parameters of the network are varied.
\\

Random neural networks, also known as random Boolean networks (RBNs), are mathematical models that capture the behavior of interconnected binary elements or nodes. Each node in the network updates its state based on the states of its neighboring nodes according to a predefined update rule.
\\

In the context of RBNs, the transition to chaos is typically studied by examining the behavior of the network as the connectivity and update rules are modified. The relevant technical details involve understanding the critical threshold for the transition, the onset of chaos, and the characterization of chaotic behavior.
\\

One important parameter that influences the transition to chaos in RBNs is the connectivity or the number of inputs received by each node. In general, there exists a critical connectivity value, denoted by $K_c$, above which the network exhibits chaotic behavior. Below $K_c$, the network tends to exhibit ordered or stable dynamics.
\\

At the critical connectivity $K_c$, the network displays a critical regime where it is poised between order and chaos. It exhibits complex behavior characterized by sensitive dependence on initial conditions, where small changes in the initial state can lead to significantly different trajectories and eventual divergence. This sensitivity to initial conditions is a hallmark of chaos.
\\

The transition to chaos in RBNs is often described by the concept of an "edge of chaos." The edge of chaos refers to the boundary between the ordered and chaotic regions in the parameter space of the network. At this boundary, the network displays a delicate balance between stability and complexity, exhibiting both regular and irregular behavior.
\\

Several measures are used to characterize the chaotic behavior in random neural networks. One commonly used measure is the attractor length or the average length of the trajectories followed by the network before cycling or settling into a fixed point. In the ordered regime, the attractor length is typically small, whereas in the chaotic regime, it becomes much larger.
\\

Other measures include the Lyapunov exponent, which quantifies the rate of divergence of nearby trajectories, and the complexity or information content of the network dynamics. These measures help quantify the degree of chaos and the sensitivity of the system to perturbations.
\\

Understanding the transition to the chaotic state in random neural networks provides insights into the dynamics and computational properties of these systems. The interplay between order and chaos in such networks is of interest due to its potential implications for information processing, adaptation, and robustness to perturbations.
\\

\section{The story of a small perturbation leaving a mark}

The phenomenon where a small perturbation induces a long-lasting effect just before the transition to the chaotic state, is known as critical slowing down. It occurs when a dynamical system approaches a critical point or undergoes a phase transition, such as the transition from an ordered to a chaotic state.
\\

To understand this effect, let's consider a simple example of a dynamical system described by a scalar variable x that evolves over time t. The dynamics of the system can be represented by a differential equation:
\\
\begin{equation}
\label{eq:3}
\begin{aligned}
dx/dt = f(x, \lambda)
\end{aligned}
\end{equation}

\\

where f(x, $\lambda$) represents the evolution function that depends on the current state x and a control parameter $\lambda$. As $\lambda$ is varied, the system undergoes a transition from an ordered state (e.g., a stable fixed point) to a chaotic state.
\\

Now, suppose the system is close to the critical point where the transition occurs, and we introduce a small perturbation $\delta$x to the system. The perturbed equation becomes:
\\

\begin{equation}
\label{eq:4}
\begin{aligned}
dx/dt = f(x, \lambda) + \delta f(x, \lambda)
\end{aligned}
\end{equation}
\\

where $\delta$f(x, $\lambda$) represents the additional contribution due to the perturbation.
\\

Near the critical point, the system becomes sensitive to perturbations, and the perturbation can have a long-lasting effect. This effect can be explained in terms of the system's relaxation time or characteristic timescale, which becomes significantly longer as the critical point is approached.
\\

When the system is far from the critical point, the relaxation time is relatively short, and the system quickly returns to its equilibrium state after a perturbation. However, as the critical point is approached, the relaxation time increases, and the system takes longer to recover from the perturbation. This slow recovery is what characterizes critical slowing down.
\\

The increased relaxation time near the critical point arises due to the presence of larger and larger fluctuations or correlations in the system. These fluctuations propagate through the system, leading to a slower convergence to equilibrium states. As a result, a small perturbation can persist in the system for an extended period before it decays or is absorbed by the chaotic dynamics.
\\

This long-lasting effect of perturbations close to the transition to the chaotic state is a consequence of the underlying dynamics becoming more correlated, exhibiting larger fluctuations, and having slower relaxation times. It is a characteristic feature observed in various physical, biological, and computational systems near critical points or phase transitions.
\\

It's worth noting that the precise manifestation and quantitative analysis of critical slowing down can depend on the specific system and its dynamics. However, the general idea of increased relaxation times and the persistence of perturbations near critical points are widely observed phenomena.
\\

\section{Fokker-Planck Equation}

The Fokker-Planck equation, also known as the Kolmogorov forward equation or the forward Chapman-Kolmogorov equation, is a partial differential equation used to describe the time evolution of the probability density function (PDF) of a stochastic process. It finds wide application in neuroscience and the study of neural networks to model and analyze the dynamics of complex systems.
\\

In the realm of neuroscience and neural networks, the Fokker-Planck equation is employed to describe how the probability distribution of neuronal membrane potentials, firing rates, or other relevant variables changes over time. It provides a probabilistic framework for understanding the system's state evolution due to stochastic inputs, noise, and the intrinsic dynamics of neurons or the network.
\\

The general form of the Fokker-Planck equation is:

\begin{equation}
\label{eq:5}
\begin{aligned}
\frac{\partial P(x,t)}{\partial t} = - \frac{\partial (A(X)P(x,t))}{\partial x} + \frac{\partial ^2 (D(X)P(x,t))}{\partial x^2}
\end{aligned}
\end{equation}
\\

Here, P(x,t) represents the probability density function of the random variable x at time t. A(x) is the drift term, accounting for the deterministic part of the system's dynamics, while D(x) is the diffusion term that captures stochastic fluctuations in the system.
\\

The drift term describes the overall direction or tendency of the system's evolution, influencing its mean behavior. The diffusion term represents the spread or dispersion of the PDF due to random processes, reflecting the stochastic nature of the system.
\\

In addition to describing the time evolution of the PDF, the Fokker-Planck equation also allows us to analyze the equilibrium distribution of the system. The equilibrium distribution, denoted as $P_{eq}$(x), represents the stationary state where the PDF no longer changes over time.
\\
In this state, the drift term vanishes (A(x) = 0), and the equation simplifies to:
\\
\begin{equation}
\label{eq:6}
\begin{aligned}
\frac{\partial P_{eq}(x,t)}{\partial t} = 0
\end{aligned}
\end{equation}
\\


\begin{equation}
\label{eq:4}
\begin{aligned}
- \frac{\partial (A(X)P(x,t))}{\partial x} + \frac{\partial ^2 (D(X)P(x,t))}{\partial x^2} = 0
\end{aligned}
\end{equation}
\\

Solving this equation provides insights into the steady-state behavior of the system, yielding the distribution of neuronal states or network variables when the system reaches equilibrium.
\\

By solving or approximating the Fokker-Planck equation, researchers can gain insights into the statistical properties of neuronal activity, characterize system stability, estimate the effects of noise, and study the system's response to external stimuli or perturbations. Furthermore, the analysis of the equilibrium distribution helps understand the long-term behavior and steady-state characteristics of the system.
\\

Numerical methods, perturbation theory, mean-field approximations, and other mathematical techniques are commonly employed to solve or approximate the Fokker-Planck equation in specific cases. These approaches provide valuable tools for understanding the complex dynamics and information processing capabilities of neural systems and network models.
\\

In summary, the Fokker-Planck equation serves as a fundamental tool in neuroscience and neural network research, enabling the probabilistic modeling and analysis of neuronal dynamics, noise effects, collective behaviors, and the characterization of the equilibrium distribution. Solutions involving the Ricciardi function can be numerically integrated for accurate simulation results.

\begin{figure}[htp]
\centering
    \includegraphics[width=16cm]{Riccardi.png}
    \caption{Courtesy Carl van Vreeswijk, the Fokker-Planck equation (Eq 8.9) in his lecture notes serves as a fundamental tool in neuroscience and neural network research, enabling the probabilistic modeling and analysis of neuronal dynamics, noise effects, collective behaviors, and the characterization of the equilibrium distribution. Solutions involving the Ricciardi function can be numerically integrated for accurate simulation results.}
    \label{fig:4}
\end{figure}

\\

\section{Programming platforms}

 Python 3 \\
 PyCharm CE on Mac \\
 MATLAB \\
 Overleaf for Latex \\
 Jupyter notebook \\
\\


\appendix
\section{Acknowledgments}

    A comprehensive analysis of the 2 research papers and lecture notes was conducted and a thorough theoretical framework has been laid down for further study on the matter. I was unable to secure meaningful data from my simulation resutls despite days of simulations and can not do the last step to computationally confirm my lemmas. However, I believe that I have conducted thorough literature review and I am quite proud of the work that I have put into learning about mean field theory, recurrent networks, perturbation and chaos. I look forward to learning more about the computational methods involved in this data collection that would yield meaningful results. I would give myself an 8/10 for the theoretical reasoning and would dock off 20 percent for the coding comparisons since they were not accurate. 

Here is a Github link to the code for extra credit:\\

\href{https://github.com/hamzaakmal98/Chaos-in-random-neural-networks.git}{Chaos in Random Neural Networks}


% Bibliography

%% [A] Recommended: using JHEP.bst file
%% \bibliographystyle{JHEP}
%% \bibliography{biblio.bib}

%% or
%% [B] Manual formatting (see below)
%% (i) We suggest to always provide author, title and journal data or doi:
%% in short all the informations that clearly identify a document.
%% (ii) please avoid comments such as "For a review'', "For some examples",
%% "and references therein" or move them in the text. In general, please leave only references in the bibliography and move all
%% accessory text in footnotes.
%% (iii) Also, please have only one work for each \bibitem.
\subsection{References}
\\
1. Brunel, N., & Hakim, V. (1999). Fast global oscillations in networks of integrate-and-fire neurons with low firing rates. Neural Computation, 11(7), 1621-1671.

2. Sompolinsky, H., Crisanti, A., & Sommers, H. J. (1988). Chaos in random neural networks. Physical Review Letters, 61(3), 259-262.

3. Touboul, J., & Destexhe, A. (2010). Can power-law scaling and neuronal avalanches arise from stochastic dynamics? PLOS ONE, 5(7), e8982.

4. Deco, G., Jirsa, V. K., & McIntosh, A. R. (2013). Resting brains never rest: computational insights into potential cognitive architectures. Trends in Neurosciences, 36(5), 268-274.

5. Bressloff, P. C., & Coombes, S. (2000). Dynamics of strongly-coupled spiking neurons. Neural Computation, 12(1), 91-129.

6. Sompolinsky, H., Crisanti, A. & Sommers, H. J. Chaos in Random Neural Networks. Phys. Rev. Lett. 61, 259–262 (1988).

7. Crisanti, A. & Sompolinsky, H. Path Integral Approach to Random Neural Networks. arXiv:1809.06042 [cond-mat] (2018).

8. Carl van Vreeswijk, The Fokker-Planck equation, 2011



\end{document}
